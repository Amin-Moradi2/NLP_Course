{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia"
      ],
      "metadata": {
        "id": "kn2Ko3mQdIAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8QgwTOdIc8RR"
      },
      "outputs": [],
      "source": [
        "import wikipedia\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import pandas as pd\n",
        "import random\n",
        "import warnings\n",
        "from bs4 import GuessedAtParserWarning\n",
        "\n",
        "# Suppress the specific BeautifulSoup warning\n",
        "warnings.filterwarnings('ignore', category=GuessedAtParserWarning)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download all required NLTK data\n",
        "def download_nltk_resources():\n",
        "    resources = [\n",
        "        'punkt',\n",
        "        'stopwords',\n",
        "        'wordnet',\n",
        "        'averaged_perceptron_tagger',\n",
        "        'punkt_tab'\n",
        "    ]\n",
        "    for resource in resources:\n",
        "        try:\n",
        "            nltk.download(resource, quiet=True)\n",
        "        except:\n",
        "            print(f\"Warning: Could not download {resource}\")\n",
        "\n",
        "# Download resources at initialization\n",
        "download_nltk_resources()"
      ],
      "metadata": {
        "id": "pgN88q0wdK7B"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WikiTextClassifier:\n",
        "    def __init__(self):\n",
        "        self.stemmer = PorterStemmer()\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    def collect_wikipedia_data(self, categories, samples_per_category=100):\n",
        "        \"\"\"\n",
        "        Collect training data from Wikipedia\n",
        "        \"\"\"\n",
        "        data = []\n",
        "        labels = []\n",
        "\n",
        "        for category, is_geographic in categories.items():\n",
        "            # Get articles from category\n",
        "            wikipedia.set_rate_limiting(True)  # Follow API etiquette\n",
        "            articles = wikipedia.search(category, results=samples_per_category)\n",
        "\n",
        "            for article in articles:\n",
        "                try:\n",
        "                    page = wikipedia.page(article)\n",
        "                    data.append(page.content)\n",
        "                    labels.append(1 if is_geographic else 0)\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        return data, labels\n",
        "\n",
        "    def preprocess_text(self, text, use_stemming=False, use_lemmatization=False):\n",
        "        \"\"\"\n",
        "        Preprocess text with optional stemming and lemmatization\n",
        "        \"\"\"\n",
        "        # Tokenization\n",
        "        tokens = word_tokenize(text.lower())\n",
        "\n",
        "        # Remove stopwords\n",
        "        tokens = [token for token in tokens if token not in self.stop_words]\n",
        "\n",
        "        # Apply stemming or lemmatization\n",
        "        if use_stemming:\n",
        "            tokens = [self.stemmer.stem(token) for token in tokens]\n",
        "        elif use_lemmatization:\n",
        "            tokens = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "    def train_classifier(self, X, y, model_type='naive_bayes',\n",
        "                        use_stemming=False, use_lemmatization=False):\n",
        "        \"\"\"\n",
        "        Train the classifier with specified options\n",
        "        \"\"\"\n",
        "        # Preprocess all texts\n",
        "        processed_texts = [\n",
        "            self.preprocess_text(text, use_stemming, use_lemmatization)\n",
        "            for text in X\n",
        "        ]\n",
        "\n",
        "        # Create vectorizer\n",
        "        vectorizer = CountVectorizer() if model_type == 'naive_bayes' else TfidfVectorizer()\n",
        "        X_vectorized = vectorizer.fit_transform(processed_texts)\n",
        "\n",
        "        # Split data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X_vectorized, y, test_size=0.2, random_state=42\n",
        "        )\n",
        "\n",
        "        # Select and train model\n",
        "        if model_type == 'naive_bayes':\n",
        "            model = MultinomialNB()\n",
        "        else:\n",
        "            model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Evaluate\n",
        "        y_pred = model.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        report = classification_report(y_test, y_pred)\n",
        "\n",
        "        return {\n",
        "            'model': model,\n",
        "            'vectorizer': vectorizer,\n",
        "            'accuracy': accuracy,\n",
        "            'report': report\n",
        "        }\n",
        "\n",
        "    def predict(self, text, model_dict, use_stemming=False, use_lemmatization=False):\n",
        "        \"\"\"\n",
        "        Predict the class of a new text\n",
        "        \"\"\"\n",
        "        processed_text = self.preprocess_text(\n",
        "            text, use_stemming, use_lemmatization\n",
        "        )\n",
        "        vectorized_text = model_dict['vectorizer'].transform([processed_text])\n",
        "        prediction = model_dict['model'].predict(vectorized_text)\n",
        "        return \"Geographic\" if prediction[0] == 1 else \"Non-Geographic\""
      ],
      "metadata": {
        "id": "HlVyiXCKdDJ1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize classifier\n",
        "classifier = WikiTextClassifier()\n",
        "\n",
        "# Define categories for training\n",
        "categories = {\n",
        "    'Geography': True,\n",
        "    'Mountains': True,\n",
        "    'Cities': True,\n",
        "    'Countries': True,\n",
        "    'Technology': False,\n",
        "    'Science': False,\n",
        "    'Art': False,\n",
        "    'Music': False\n",
        "}\n",
        "\n",
        "print(\"Collecting Wikipedia data...\")\n",
        "X, y = classifier.collect_wikipedia_data(categories, samples_per_category=70)\n",
        "\n",
        "# Train different models and compare results\n",
        "models = {\n",
        "    'naive_bayes': {\n",
        "        'basic': {},\n",
        "        'stemming': {'use_stemming': True},\n",
        "        'lemmatization': {'use_lemmatization': True}\n",
        "    },\n",
        "    'logistic_regression': {\n",
        "        'basic': {},\n",
        "        'stemming': {'use_stemming': True},\n",
        "        'lemmatization': {'use_lemmatization': True}\n",
        "    }\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "for model_type in models:\n",
        "    for variant, params in models[model_type].items():\n",
        "        print(f\"\\nTraining {model_type} with {variant} configuration...\")\n",
        "        result = classifier.train_classifier(\n",
        "            X, y, model_type=model_type, **params\n",
        "        )\n",
        "        results[f\"{model_type}_{variant}\"] = result\n",
        "        print(f\"Accuracy: {result['accuracy']:.4f}\")\n",
        "        print(\"Classification Report:\")\n",
        "        print(result['report'])\n",
        "\n",
        "# Test the best model with some example texts\n",
        "best_model = max(results.items(), key=lambda x: x[1]['accuracy'])\n",
        "print(f\"\\nBest model: {best_model[0]}\")\n",
        "\n",
        "# Test examples\n",
        "test_texts = [\n",
        "    \"Mount Everest is the highest mountain above sea level located in the Himalayas.\",\n",
        "    \"The theory of relativity was proposed by Albert Einstein in 1915.\",\n",
        "]\n",
        "\n",
        "for text in test_texts:\n",
        "    prediction = classifier.predict(text, best_model[1])\n",
        "    print(f\"\\nText: {text}\")\n",
        "    print(f\"Prediction: {prediction}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-thnKbGdaJd",
        "outputId": "d0fd2a89-791e-44ca-9ac4-9fee10550e4c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Wikipedia data...\n",
            "\n",
            "Training naive_bayes with basic configuration...\n",
            "Accuracy: 0.9583\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.98      0.96        49\n",
            "           1       0.98      0.94      0.96        47\n",
            "\n",
            "    accuracy                           0.96        96\n",
            "   macro avg       0.96      0.96      0.96        96\n",
            "weighted avg       0.96      0.96      0.96        96\n",
            "\n",
            "\n",
            "Training naive_bayes with stemming configuration...\n",
            "Accuracy: 0.9583\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.98      0.96        49\n",
            "           1       0.98      0.94      0.96        47\n",
            "\n",
            "    accuracy                           0.96        96\n",
            "   macro avg       0.96      0.96      0.96        96\n",
            "weighted avg       0.96      0.96      0.96        96\n",
            "\n",
            "\n",
            "Training naive_bayes with lemmatization configuration...\n",
            "Accuracy: 0.9583\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.98      0.96        49\n",
            "           1       0.98      0.94      0.96        47\n",
            "\n",
            "    accuracy                           0.96        96\n",
            "   macro avg       0.96      0.96      0.96        96\n",
            "weighted avg       0.96      0.96      0.96        96\n",
            "\n",
            "\n",
            "Training logistic_regression with basic configuration...\n",
            "Accuracy: 0.9896\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99        49\n",
            "           1       1.00      0.98      0.99        47\n",
            "\n",
            "    accuracy                           0.99        96\n",
            "   macro avg       0.99      0.99      0.99        96\n",
            "weighted avg       0.99      0.99      0.99        96\n",
            "\n",
            "\n",
            "Training logistic_regression with stemming configuration...\n",
            "Accuracy: 0.9896\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99        49\n",
            "           1       1.00      0.98      0.99        47\n",
            "\n",
            "    accuracy                           0.99        96\n",
            "   macro avg       0.99      0.99      0.99        96\n",
            "weighted avg       0.99      0.99      0.99        96\n",
            "\n",
            "\n",
            "Training logistic_regression with lemmatization configuration...\n",
            "Accuracy: 0.9896\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99        49\n",
            "           1       1.00      0.98      0.99        47\n",
            "\n",
            "    accuracy                           0.99        96\n",
            "   macro avg       0.99      0.99      0.99        96\n",
            "weighted avg       0.99      0.99      0.99        96\n",
            "\n",
            "\n",
            "Best model: logistic_regression_basic\n",
            "\n",
            "Text: Mount Everest is the highest mountain above sea level located in the Himalayas.\n",
            "Prediction: Geographic\n",
            "\n",
            "Text: The theory of relativity was proposed by Albert Einstein in 1915.\n",
            "Prediction: Non-Geographic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f5joPLXcdnQi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}