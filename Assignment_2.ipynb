{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6roT0_Vr77k",
        "outputId": "5460fe5a-7c9a-47f9-c8a7-0baf267ef271"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Faker\n",
            "  Downloading Faker-35.2.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.11/dist-packages (from Faker) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from Faker) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.4->Faker) (1.17.0)\n",
            "Downloading Faker-35.2.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Faker\n",
            "Successfully installed Faker-35.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install Faker"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from faker import Faker\n",
        "import random\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.cluster.util import cosine_distance\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import os\n"
      ],
      "metadata": {
        "id": "ObqwobvcQP-c"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating Text"
      ],
      "metadata": {
        "id": "t8vi_WO-_Yua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextGenerator:\n",
        "    def __init__(self, seed=None):\n",
        "        self.faker = Faker()\n",
        "        if seed:\n",
        "            Faker.seed(seed)\n",
        "            random.seed(seed)\n",
        "\n",
        "    def generate_paragraph(self, num_sentences=5, min_words=15, max_words=25):\n",
        "        \"\"\"Generate a paragraph with longer sentences\"\"\"\n",
        "        sentences = []\n",
        "        for _ in range(num_sentences):\n",
        "            # Generate longer sentences by combining multiple faker sentences\n",
        "            num_words = random.randint(min_words, max_words)\n",
        "            words = []\n",
        "            while len(words) < num_words:\n",
        "                words.extend(self.faker.sentence().split()[:-1])  # Remove period\n",
        "            sentence = ' '.join(words[:num_words]) + '.'\n",
        "            sentences.append(sentence)\n",
        "        return ' '.join(sentences)\n",
        "\n",
        "    def generate_document(self, target_size, min_sentences=5, max_sentences=10):\n",
        "        \"\"\"Generate a document targeting a specific size in tokens\"\"\"\n",
        "        document = []\n",
        "        current_size = 0\n",
        "\n",
        "        while current_size < target_size:\n",
        "            num_sentences = random.randint(min_sentences, max_sentences)\n",
        "            paragraph = self.generate_paragraph(num_sentences, min_words=15, max_words=25)\n",
        "            document.append(paragraph)\n",
        "            current_size += len(paragraph.split())\n",
        "\n",
        "        return '\\n\\n'.join(document)\n",
        "\n",
        "    def generate_test_documents(self, num_docs=2, size_multiplier=1):\n",
        "        \"\"\"Generate test documents with different sizes\"\"\"\n",
        "        documents = []\n",
        "        for i in range(num_docs):\n",
        "            if i == 0:\n",
        "                # First document: style reference (~2000-3000 tokens)\n",
        "                target_size = 2500 * size_multiplier\n",
        "            else:\n",
        "                # Second document: document to summarize (~6000-8000 tokens)\n",
        "                target_size = 7000 * size_multiplier\n",
        "\n",
        "            doc = self.generate_document(target_size)\n",
        "            documents.append(doc)\n",
        "        return documents"
      ],
      "metadata": {
        "id": "Xiz-A1cC-32O"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_nltk():\n",
        "    \"\"\"Initialize NLTK resources safely\"\"\"\n",
        "    try:\n",
        "        # Download required NLTK data\n",
        "        nltk.download('punkt_tab')  # Add punkt_tab specifically\n",
        "        nltk.download('punkt')\n",
        "        nltk.download('stopwords')\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Failed to download NLTK resources: {str(e)}\")\n",
        "\n",
        "    # Verify the resources are available\n",
        "    try:\n",
        "        nltk.data.find('tokenizers/punkt_tab/english')  # Updated path\n",
        "        nltk.data.find('tokenizers/punkt')\n",
        "        nltk.data.find('corpora/stopwords')\n",
        "    except LookupError as e:\n",
        "        # If punkt_tab is not found, we can still proceed with punkt\n",
        "        if 'punkt_tab' in str(e):\n",
        "            print(\"Warning: punkt_tab not found, falling back to punkt\")\n",
        "        else:\n",
        "            print(f\"Error: Required NLTK resources not found: {str(e)}\")\n",
        "            raise"
      ],
      "metadata": {
        "id": "7b1J61opQBvP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyzing Text"
      ],
      "metadata": {
        "id": "EAJUADS3_b3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StyleAnalyzer:\n",
        "    \"\"\"Analyzes and matches text style characteristics\"\"\"\n",
        "    def __init__(self):\n",
        "        initialize_nltk()\n",
        "\n",
        "    def analyze_style(self, text):\n",
        "        \"\"\"Extract style characteristics from text\"\"\"\n",
        "        sentences = sent_tokenize(text)\n",
        "        words = word_tokenize(text)\n",
        "\n",
        "        return {\n",
        "            'avg_sentence_length': sum(len(word_tokenize(s)) for s in sentences) / len(sentences),\n",
        "            'sentence_lengths': [len(word_tokenize(s)) for s in sentences],\n",
        "            'avg_word_length': sum(len(w) for w in words) / len(words)\n",
        "        }\n",
        "\n",
        "    def match_style(self, sentences, ref_style):\n",
        "        \"\"\"Adjust sentences to match reference style\"\"\"\n",
        "        target_avg_length = ref_style['avg_sentence_length']\n",
        "        adjusted_sentences = []\n",
        "\n",
        "        for sentence in sentences:\n",
        "            words = word_tokenize(sentence)\n",
        "            current_length = len(words)\n",
        "\n",
        "            if current_length > target_avg_length * 1.5:\n",
        "                # Split long sentences\n",
        "                mid = len(words) // 2\n",
        "                adjusted_sentences.extend([\n",
        "                    ' '.join(words[:mid]) + '.',\n",
        "                    ' '.join(words[mid:]) + '.'\n",
        "                ])\n",
        "            elif current_length < target_avg_length * 0.5 and adjusted_sentences:\n",
        "                # Combine short sentences when possible\n",
        "                prev = adjusted_sentences.pop()\n",
        "                combined = f\"{prev[:-1]} and {sentence}\"\n",
        "                adjusted_sentences.append(combined)\n",
        "            else:\n",
        "                adjusted_sentences.append(sentence)\n",
        "\n",
        "        return adjusted_sentences"
      ],
      "metadata": {
        "id": "rt2tnq__-8Cl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Summarizer:\n",
        "    def __init__(self, context_window_size=4000):\n",
        "        self.context_window_size = context_window_size\n",
        "        initialize_nltk()\n",
        "        try:\n",
        "            self.stop_words = set(stopwords.words('english'))\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Failed to load stopwords, using empty set: {str(e)}\")\n",
        "            self.stop_words = set()\n",
        "        self.style_analyzer = StyleAnalyzer()\n",
        "\n",
        "    def get_token_count(self, text):\n",
        "        return len(word_tokenize(text))\n",
        "\n",
        "    def sent_tokenize(self, text):\n",
        "        \"\"\"Wrapper for NLTK's sent_tokenize\"\"\"\n",
        "        return sent_tokenize(text)\n",
        "\n",
        "    def sentence_similarity(self, sent1, sent2):\n",
        "        words1 = [word.lower() for word in word_tokenize(sent1) if word.lower() not in self.stop_words]\n",
        "        words2 = [word.lower() for word in word_tokenize(sent2) if word.lower() not in self.stop_words]\n",
        "\n",
        "        all_words = list(set(words1 + words2))\n",
        "\n",
        "        vector1 = [0] * len(all_words)\n",
        "        vector2 = [0] * len(all_words)\n",
        "\n",
        "        for word in words1:\n",
        "            vector1[all_words.index(word)] += 1\n",
        "        for word in words2:\n",
        "            vector2[all_words.index(word)] += 1\n",
        "\n",
        "        return 1 - cosine_distance(vector1, vector2)\n",
        "\n",
        "    def generate_summary(self, text, style_reference=None, target_length=None):\n",
        "        \"\"\"Generate summary with style matching\"\"\"\n",
        "        sentences = self.sent_tokenize(text)\n",
        "        if len(sentences) <= 1:\n",
        "            return text\n",
        "\n",
        "        # Generate basic summary\n",
        "        similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
        "        for idx1 in range(len(sentences)):\n",
        "            for idx2 in range(len(sentences)):\n",
        "                if idx1 != idx2:\n",
        "                    similarity_matrix[idx1][idx2] = self.sentence_similarity(sentences[idx1], sentences[idx2])\n",
        "\n",
        "        nx_graph = nx.from_numpy_array(similarity_matrix)\n",
        "        scores = nx.pagerank(nx_graph)\n",
        "        ranked_sentences = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)\n",
        "\n",
        "        # Select sentences based on target length\n",
        "        if target_length:\n",
        "            current_length = 0\n",
        "            summary_sentences = []\n",
        "            for _, sentence in ranked_sentences:\n",
        "                current_length += len(word_tokenize(sentence))\n",
        "                if current_length > target_length:\n",
        "                    break\n",
        "                summary_sentences.append(sentence)\n",
        "        else:\n",
        "            num_sentences = max(1, int(len(sentences) * 0.3))\n",
        "            summary_sentences = [s for _, s in ranked_sentences[:num_sentences]]\n",
        "\n",
        "        # Apply style matching if reference is provided\n",
        "        if style_reference:\n",
        "            ref_style = self.style_analyzer.analyze_style(style_reference)\n",
        "            summary_sentences = self.style_analyzer.match_style(summary_sentences, ref_style)\n",
        "\n",
        "        return ' '.join(summary_sentences)\n",
        "\n",
        "    def hierarchical_summarize(self, text, style_reference=None, target_length=None):\n",
        "        \"\"\"Hierarchical summarization with style matching\"\"\"\n",
        "        current_text = text\n",
        "        while self.get_token_count(current_text) > self.context_window_size:\n",
        "            sentences = self.sent_tokenize(current_text)\n",
        "            chunks = []\n",
        "            current_chunk = []\n",
        "            current_length = 0\n",
        "\n",
        "            for sentence in sentences:\n",
        "                sentence_length = self.get_token_count(sentence)\n",
        "                if current_length + sentence_length > self.context_window_size:\n",
        "                    if current_chunk:\n",
        "                        chunks.append(' '.join(current_chunk))\n",
        "                    current_chunk = [sentence]\n",
        "                    current_length = sentence_length\n",
        "                else:\n",
        "                    current_chunk.append(sentence)\n",
        "                    current_length += sentence_length\n",
        "\n",
        "            if current_chunk:\n",
        "                chunks.append(' '.join(current_chunk))\n",
        "\n",
        "            # Summarize each chunk with style matching\n",
        "            chunk_target = target_length // len(chunks) if target_length else None\n",
        "            summaries = [\n",
        "                self.generate_summary(\n",
        "                    chunk,\n",
        "                    style_reference=style_reference,\n",
        "                    target_length=chunk_target\n",
        "                ) for chunk in chunks\n",
        "            ]\n",
        "            current_text = ' '.join(summaries)\n",
        "\n",
        "        return current_text"
      ],
      "metadata": {
        "id": "7rHgwLlR-8GF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_text(text, filename):\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        f.write(text)\n",
        "\n",
        "def calculate_proportional_length(ref_length, doc_length, context_window_size):\n",
        "    \"\"\"Calculate target length proportionally\"\"\"\n",
        "    ratio = ref_length / doc_length\n",
        "    return min(context_window_size, int(doc_length * ratio))"
      ],
      "metadata": {
        "id": "uhTPGLnF-8JW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "W4HHH9CV_kGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize components\n",
        "generator = TextGenerator(seed=42)\n",
        "summarizer = Summarizer(context_window_size=4000)\n",
        "\n",
        "# Generate test documents\n",
        "docs = generator.generate_test_documents(num_docs=2, size_multiplier=2)\n",
        "style_reference = docs[0]  # First document is style reference\n",
        "document_to_summarize = docs[1]  # Second document needs summarization\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs('output', exist_ok=True)\n",
        "\n",
        "# Step 1: Measure document lengths\n",
        "ref_length = summarizer.get_token_count(style_reference)\n",
        "doc_length = summarizer.get_token_count(document_to_summarize)\n",
        "\n",
        "print(\"-------------------------\\n\")\n",
        "print(f\"Style reference length: {ref_length} tokens\")\n",
        "print(f\"Document to summarize length: {doc_length} tokens\")\n",
        "\n",
        "# Step 2: Compute target length proportionally\n",
        "target_length = calculate_proportional_length(\n",
        "    ref_length,\n",
        "    doc_length,\n",
        "    summarizer.context_window_size\n",
        ")\n",
        "print(f\"Target summary length: {target_length} tokens\")\n",
        "\n",
        "# Save original documents\n",
        "save_text(style_reference, 'output/style_reference.txt')\n",
        "save_text(document_to_summarize, 'output/document_to_summarize.txt')\n",
        "\n",
        "# Generate summary with style matching\n",
        "if doc_length > summarizer.context_window_size:\n",
        "    final_summary = summarizer.hierarchical_summarize(\n",
        "        document_to_summarize,\n",
        "        style_reference=style_reference,\n",
        "        target_length=target_length\n",
        "    )\n",
        "else:\n",
        "    final_summary = summarizer.generate_summary(\n",
        "        document_to_summarize,\n",
        "        style_reference=style_reference,\n",
        "        target_length=target_length\n",
        "    )\n",
        "\n",
        "# Save the final summary\n",
        "save_text(final_summary, 'output/final_summary.txt')\n",
        "final_length = summarizer.get_token_count(final_summary)\n",
        "print(f\"Final summary length: {final_length} tokens\")\n",
        "print(\"Process completed successfully!\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23YMQf42_POm",
        "outputId": "17a9bb7b-4634-4dcf-8324-31a86f9e68f1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------\n",
            "\n",
            "Style reference length: 5361 tokens\n",
            "Document to summarize length: 14852 tokens\n",
            "Target summary length: 4000 tokens\n",
            "Final summary length: 3959 tokens\n",
            "Process completed successfully!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}